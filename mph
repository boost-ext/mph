//
// Copyright (c) 2024 Kris Jusiak (kris at jusiak dot net)
//
// Distributed under the Boost Software License, Version 1.0.
// (See accompanying file LICENSE_1_0.txt or copy at
// http://www.boost.org/LICENSE_1_0.txt)
//
#ifndef MPH
#define MPH 2'5'0 // SemVer
#pragma GCC system_header

/**
 * [Minimal] Perfect hash library (https://github.com/boost-ext/mph)
 */
namespace mph::inline v2_5_0 {
using u8   = __UINT8_TYPE__;
using u16  = __UINT16_TYPE__;
using u32  = __UINT32_TYPE__;
using u64  = __UINT64_TYPE__;
using u128 = unsigned __int128;

namespace utility {
template<class T1, class T2>
struct compressed_pair {
  using first_type = T1;
  using second_type = T2;
  [[no_unique_address]] T1 first;
  [[no_unique_address]] T2 second;
};
template<class T1, class T2> compressed_pair(T1, T2) -> compressed_pair<T1, T2>;

template<class T, u32 Size>
struct array {
  using value_type = T;
  [[nodiscard]] constexpr const T* data() const noexcept { return data_; }
  [[nodiscard]] constexpr T* data() noexcept { return data_; }
  [[nodiscard]] constexpr auto begin() const noexcept { return &data_[0]; }
  [[nodiscard]] constexpr auto begin() noexcept { return &data_[0]; }
  [[nodiscard]] constexpr auto end() const noexcept { return &data_[0] + Size; }
  [[nodiscard]] constexpr auto end() noexcept { return &data_[0] + Size; }
  [[nodiscard]] constexpr auto size() const noexcept { return Size; }
  [[nodiscard]] constexpr const auto& operator[](u32 i) const noexcept { return data_[i]; }
  [[nodiscard]] constexpr auto& operator[](u32 i) noexcept { return data_[i]; }
  constexpr void fill(const T& value) noexcept { for (auto i = 0u; i < Size; ++i) { data_[i] = value; } }
  T data_[Size];
};
template<class T, class... Ts> array(T, Ts...) -> array<T, 1u + sizeof...(Ts)>;
} // namespace utility

namespace type_traits {
template<class T, u32 alignment>
struct aligned {
  using type = struct alignas(alignment) a  : T { };
  static_assert(alignment == alignof(type));
};
template<class T> struct aligned<T, 0u> { using type = T; };
template<class T, u32 alignment>
using aligned_t = typename aligned<T, alignment>::type;

namespace detail {
template <bool> struct conditional;
template <> struct conditional<false> { template <class, class T> using fn = T; };
template <> struct conditional<true>  { template <class T, class> using fn = T; };
template<class T> auto value_type(const T&) -> typename T::value_type;
} // namespace detail

template <bool B, class T, class F>
using conditional_t = typename detail::conditional<B>::template fn<T, F>;

template<const auto& kv>
using value_type_t = decltype(detail::value_type(kv));

template<auto v> struct constant {
  static constexpr auto value = v;
  constexpr operator auto() const noexcept { return value; }
};
template<auto v> inline constexpr constant<v> constant_v{};
} // namespace type_traits

namespace concepts {
template<class T> concept integral = requires {
  reinterpret_cast<T>(T(0)) + T();
};

template<const auto& kv> concept range = requires(u32 n) {
  kv.size(); kv.begin(); kv.end(); kv[n];
};

template<auto c> concept config = requires {
  c.key_in_set_probability; c.group_size; c.lookup_table_alignment;
} and (
  c.key_in_set_probability >= 0u and c.key_in_set_probability <= 100u and
  c.group_size >= 1u and
  c.lookup_table_alignment >= 0u and not (c.lookup_table_alignment % 2)
);
} // namespace concepts

namespace detail {
template<class T>
[[nodiscard]] [[gnu::always_inline]] constexpr auto pext(const T a, const auto mask) noexcept -> T {
  // https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html#text=pext
  if (constexpr T size = sizeof(T) * __CHAR_BIT__; __builtin_is_constant_evaluated()) {
    T result{};
    T m = mask;
    auto k = 0u;
    for (T i{}; i < size; ++i) {
      if (m & 1) result |= ((a >> i) & 1) << k++;
      m >>= 1;
    }
    return result;
  }
  #if defined(__BMI2__)
  else if constexpr (requires { u32{mask}; }) { return __builtin_ia32_pext_si(a, mask); }
  else if constexpr (requires { u64{mask}; }) { return __builtin_ia32_pext_di(a, mask); }
  #endif
  else if constexpr (requires { mask.value; }) {
    // https://github.com/intel/compile-time-init-build/blob/main/include/lookup/pseudo_pext_lookup.hpp
    return [&] {
      static constexpr auto nbits = __builtin_popcountl(mask);
      static constexpr auto cbits = size - nbits -
        (sizeof(mask.value) <= sizeof(u32) ? __builtin_clz(mask) : __builtin_clzl(mask));
      static constexpr auto coefficient = [&] {
        auto set = false;
        auto dst = cbits;
        T result{};
        for (auto i = 0u; i < size; ++i) {
          const auto curr = ((T(1) << i) & mask) != T();
          if (curr and not set) result = result | (T(1) << (dst - i));
          dst += curr;
          set = curr;
        }
        return result;
      }();
      return ((a & mask) * coefficient >> cbits) & ((T(1) << nbits) - T(1));
    }();
  }
  return {};
}

template<class T, u32 N>
[[nodiscard]] constexpr auto mask(const auto& v) noexcept -> T {
  utility::array<T, decltype(v){}.size()> vs;
  T max{};
  u32 size{};
  for (auto i = 0u; i < vs.size(); ++i) {
    if (not v[i].first) break;
    if (vs[i] = v[i].first; vs[i] > max) max = vs[i];
    ++size;
  }
  if (size <= 1u) {
    return {};
  }
  constexpr auto H = (N * vs.size()) << 1u;
  constexpr auto N_ = N - 1u;
  const auto clz = sizeof(max) <= sizeof(u32) ? __builtin_clz(max) : __builtin_clzl(max);
  const auto nbits = sizeof(T) * __CHAR_BIT__ - clz - 1u;
  utility::array<T, H> hashed;
  T mask = ((T(1) << nbits) - 1u);
  for (int i = nbits; i >= 0; --i) {
    mask &= ~(T(1) << i);
    hashed = {};
    for (auto j = 0u; j < size; ++j) {
      const auto masked = (vs[j] & mask) + 1u;
      auto slot = masked % H;
      auto n = N_;
      auto found = false;
      while (hashed[slot]) {
        if (hashed[slot] == masked and not n--) {
          found = true;
          break;
        }
        slot = (slot + 1u) % H;
      }
      if (found) {
        mask |= (T(1) << i);
        break;
      }
      hashed[slot] = masked;
    }
  }
  return mask;
}
} // namespace detail

template<class T>
[[nodiscard]] [[gnu::always_inline]] constexpr auto to(const auto& data) noexcept -> T {
  if (__builtin_is_constant_evaluated()) {
    if constexpr (requires { data.data(); data.size(); }) {
      T t{};
      for (auto i = 0u; i < data.size(); ++i) {
        t = (t << __CHAR_BIT__) | data.data()[data.size() - i - 1u];
      }
      return t;
    } else if constexpr (requires(u32 n) { data[n]; }) {
      u32 size{};
      auto ptr = data;
      while (*ptr++) size++;
      T t{};
      for (auto i = 0u; i < size; ++i) {
        t = (t << __CHAR_BIT__) | data[size - i - 1u];
      }
      return t;
    } else {
      return data;
    }
  } else if constexpr (requires { T(data); data + decltype(data){}; }) {
    return data;
  } else if constexpr (requires { data.data(); data.size(); }) {
    #if not defined(MPH_PAGE_SIZE)
    #define MPH_PAGE_SIZE 4096u
    #endif
    if constexpr (requires { []<template<class, auto> class T_, class _, auto size>(T_<_, size>){}(data); }) {
      return [&]<template<class, auto> class T_, class _, auto size>(T_<_, size>) {
        if constexpr (not size) {
          return T{};
        } else if constexpr (size == sizeof(T)) {
          return *__builtin_bit_cast(const T*, data.data());
        } else if constexpr (size <= sizeof(T)) {
          T t{};
          __builtin_memcpy(&t, data.data(), size);
          return t;
        }
      }(data);
    } else if constexpr (MPH_PAGE_SIZE) {
      if constexpr (MPH_PAGE_SIZE > 0u) {
        // https://github.com/bminor/glibc/blob/master/sysdeps/generic/memcopy.h#L162
        if ((u64(data.data()) & (MPH_PAGE_SIZE - 1ul)) > (MPH_PAGE_SIZE - sizeof(T))) [[unlikely]] { // page boundry
          return [&data] [[gnu::cold]] { // unlikely path
            T t{};
            __builtin_memcpy(&t, data.data(), data.size());
            return t;
          }();
        }
      }
      T t;
      __builtin_memcpy(&t, data.data(), sizeof(t)); // not at page boundry
      const auto index = T(data.size() * __CHAR_BIT__);
      #if defined(__BMI2__)
      if constexpr (sizeof(t) <= sizeof(u32)) {
        return __builtin_ia32_bzhi_si(t, index);
      } else if constexpr (sizeof(t) <= sizeof(u64)) {
        return __builtin_ia32_bzhi_di(t, index);
      } else
      #endif
        return t & ((T(1) << index) - T(1));
    } else {
      T t{};
      __builtin_memcpy(&t, data.data(), data.size()); // slow path
      return t;
    }
  }
  return {};
}

template<const auto& kv>
struct config {
  /// 0         - none of the input key can be found in the kv
  /// (0, 50)   - input key is unlikely to be found in the kv
  /// 50        - unpredictable (default)
  /// (50, 100) - input key is likely to be found in the kv
  /// 100       - all input key can be found in the kv
  u8 key_in_set_probability{50};

  /// 1 - one element per group (faster but larger memory footprint)
  /// N - n elements per group  (slower but smaller memory footprint)
  u32 group_size{
    []() -> u32 {
      constexpr auto size_of = [] {
        if constexpr (requires(u32 n) { kv[n].first; }) {
          return sizeof(kv[0].first);
        } else {
          return sizeof(kv[0]);
        }
      }();
      switch (size_of) {
        case sizeof(u8):   return kv.size() <= sizeof(u8)   * (1u << 10u) ? 1u : 4u * sizeof(u8);
        case sizeof(u16):  return kv.size() <= sizeof(u16)  * (1u << 10u) ? 1u : 4u * sizeof(u16);
        case sizeof(u32):  return kv.size() <= sizeof(u32)  * (1u << 10u) ? 1u : 4u * sizeof(u32);
        case sizeof(u64):  return kv.size() <= sizeof(u64)  * (1u << 7u)  ? 1u : 2u * sizeof(u64);
        case sizeof(u128): return kv.size() <= sizeof(u128) * (1u << 7u)  ? 1u : 2u * sizeof(u128);
      }
    }()
  };

  /// 0 - no alignment
  /// N - alignas(N) lookup_table
  u32 lookup_table_alignment{};
};

template<const auto& kv, auto cfg = config<kv>{}, class T = type_traits::value_type_t<kv>>
  requires concepts::range<kv> and concepts::config<cfg> and concepts::integral<decltype(kv[0].first)> and (
    kv.size() == 0u or
    cfg.key_in_set_probability == 0u
  )
[[nodiscard]] [[gnu::always_inline]]
constexpr auto hash([[maybe_unused]] const auto& key) noexcept -> T::second_type {
  return {};
}

template<const auto& kv, auto cfg = config<kv>{}, class T = type_traits::value_type_t<kv>>
  requires concepts::range<kv> and concepts::config<cfg> and concepts::integral<decltype(kv[0].first)> and (
    kv.size() == 1u and
    cfg.key_in_set_probability > 0u and cfg.key_in_set_probability <= 100u
  )
[[nodiscard]] [[gnu::always_inline]]
constexpr auto hash(const auto& key) noexcept -> T::second_type {
  using key_type = T::first_type;
  using value_type = T::second_type;
  if constexpr (cfg.key_in_set_probability == 100u) {
    return kv[0].second;
  } else {
    return to<key_type>(key) == kv[0].first ? kv[0].second : value_type{};
  }
}

template<const auto& kv, auto cfg = config<kv>{}, class T = type_traits::value_type_t<kv>>
  requires concepts::range<kv> and concepts::config<cfg> and concepts::integral<decltype(kv[0].first)> and (
    kv.size() > 1u and
    cfg.key_in_set_probability > 0u and cfg.key_in_set_probability <= 100u and
    cfg.group_size == 1u
  )
[[nodiscard]] [[gnu::always_inline]]
constexpr auto hash(const auto& key) noexcept -> T::second_type {
  using key_type = T::first_type;
  if (__builtin_is_constant_evaluated()) {
    for (auto&& lhs = to<key_type>(key); const auto& [key, value] : kv) { if (lhs == key) { return value; } }
  } else {
    return [&] {
      using value_type = T::second_type;
      using key_value_type = type_traits::conditional_t<
        cfg.key_in_set_probability == 100u,
        value_type,
        utility::compressed_pair<key_type, value_type>
      >;
      using mask_type = type_traits::conditional_t<sizeof(key_type) <= sizeof(u32), u32, u64>;
      static constexpr const auto mask = type_traits::constant_v<mask_type(detail::mask<key_type, cfg.group_size>(kv))>;
      static constexpr const auto lookup = [] {
        constexpr auto fill = [] {
          if constexpr (cfg.key_in_set_probability == 100u) {
            return []([[maybe_unused]] const auto& key, const auto& value) { return value; };
          } else {
            return [](const auto& key, const auto& value) { return utility::compressed_pair{key, value}; };
          }
        }();
        type_traits::aligned_t<
          utility::array<key_value_type, mask_type(1) << __builtin_popcountl(mask)>,
          cfg.lookup_table_alignment
        > lookup{};
        for (const auto& [key, value] : kv) { lookup[detail::pext(key, mask)] = fill(key, value); }
        return lookup;
      }();
      auto&& lhs = to<key_type>(key);
      auto&& rhs = lookup[detail::pext(lhs, mask)];
      if constexpr (requires { rhs.first; rhs.second; }) {
        #if __has_builtin(__builtin_unpredictable)
        if constexpr (cfg.key_in_set_probability == 50u) {
         return __builtin_unpredictable(lhs == rhs.first) ? rhs.second : value_type{};
        }
        else
        #endif
        if constexpr (cfg.key_in_set_probability == 50u) {
          return (lhs == rhs.first) * value_type(rhs.second); // cmov
        } else {
          return __builtin_expect_with_probability(lhs == rhs.first, 1,
            [] { return float(cfg.key_in_set_probability) / float(100u); }()) ? rhs.second : value_type{};
        }
      } else {
        return rhs;
      }
    }();
  }
  return {};
}

template<const auto& kv, auto cfg = config<kv>{}, class T = type_traits::value_type_t<kv>>
  requires concepts::range<kv> and concepts::config<cfg> and concepts::integral<decltype(kv[0].first)> and (
    kv.size() > 1u and
    cfg.key_in_set_probability > 0u and cfg.key_in_set_probability <= 100u and
    cfg.group_size > 1u
  )
[[nodiscard]] [[gnu::always_inline]]
constexpr auto hash(const auto& key) noexcept -> T::second_type {
  using key_type = T::first_type;
  if (__builtin_is_constant_evaluated()) {
    for (auto&& lhs = to<key_type>(key); const auto& [key, value] : kv) { if (lhs == key) { return value; } }
  } else {
    return [&] {
      using value_type = T::second_type;
      using key_value_type = type_traits::conditional_t<
        cfg.key_in_set_probability == 100u,
        value_type,
        utility::compressed_pair<key_type, value_type>
      >;
      using mask_type = type_traits::conditional_t<sizeof(key_type) <= sizeof(u32), u32, u64>;
      static constexpr const auto mask = type_traits::constant_v<mask_type(detail::mask<key_type, cfg.group_size>(kv))>;
      static constexpr const auto lookup = [] {
        utility::array<
          utility::array<utility::compressed_pair<key_type, value_type>, cfg.group_size>,
          mask_type(1) << __builtin_popcountl(mask)
        > lookup{};
        for (const auto& [key, value] : kv) {
          auto& slot = lookup[detail::pext(key, mask)];
          auto n = 0u;
          while (slot[n].first) n++;
          slot[n] = utility::compressed_pair{key, value};
        }
        return lookup;
      }();
      static constexpr const auto masks_offsets = [] {
        type_traits::aligned_t<
          utility::array<utility::compressed_pair<u32, mask_type>, lookup.size()>,
          cfg.lookup_table_alignment
        > masks_offsets{};
        u32 offset{};
        for (auto i = 0u; i < lookup.size(); ++i) {
          if (const auto& slot = lookup[i]; slot[0].first) {
            const auto mask = detail::mask<key_type, 1u>(slot);
            masks_offsets[i] = utility::compressed_pair{offset, mask};
            offset += (mask_type(1) << __builtin_popcountl(mask));
          }
        }
        return utility::compressed_pair{masks_offsets, offset};
      }();
      static constexpr const auto lookups = [] {
        constexpr auto fill = [] {
          if constexpr (cfg.key_in_set_probability == 100u) {
            return []([[maybe_unused]] const auto& key, const auto& value) { return value; };
          } else {
            return [](const auto& key, const auto& value) { return utility::compressed_pair{key, value}; };
          }
        }();
        type_traits::aligned_t<
          utility::array<key_value_type, masks_offsets.second>,
          cfg.lookup_table_alignment
        > lookups{};
        for (auto i = 0u; i < lookup.size(); ++i) {
          if (auto& mo = masks_offsets.first[i]; lookup[i][0].first) {
            for (const auto& [key, value] : lookup[i]) {
              if (not key) break;
              lookups[mo.first + detail::pext(key, mo.second)] = fill(key, value);
            }
          }
        }
        return lookups;
      }();
      auto&& lhs = to<key_type>(key);
      auto&& mdx = masks_offsets.first[detail::pext(lhs, mask)];
      auto&& rhs = *static_cast<const key_value_type*>(&lookups[mdx.first + detail::pext(lhs, mdx.second)]);
      if constexpr (requires { rhs.first; rhs.second; }) {
        #if __has_builtin(__builtin_unpredictable)
        if constexpr (cfg.key_in_set_probability == 50u) {
         return __builtin_unpredictable(lhs == rhs.first) ? rhs.second : value_type{};
        }
        else
        #endif
        if constexpr (cfg.key_in_set_probability == 50u) {
          return (lhs == rhs.first) * value_type(rhs.second); // cmov
        } else {
          return __builtin_expect_with_probability(lhs == rhs.first, 1,
            [] { return float(cfg.key_in_set_probability) / float(100u); }()) ? rhs.second : value_type{};
        }
      } else {
        return rhs;
      }
    }();
  }
  return {};
}

namespace detail {
template<const auto& keys>
constexpr auto kv() requires requires { keys[0].size(); } {
  constexpr auto max_len = [] {
    u32 len{};
    for (const auto& key : keys) { if (key.size() > len) { len = key.size(); } }
    return len;
  }();

  using key_type = decltype([] {
        if constexpr (max_len <= sizeof(u8)) { return u8{}; }
   else if constexpr (max_len <= sizeof(u16)) { return u16{}; }
   else if constexpr (max_len <= sizeof(u32)) { return u32{}; }
   else if constexpr (max_len <= sizeof(u64)) { return u64{}; }
   else if constexpr (max_len <= sizeof(u128)) { return u128{}; }
  }());

  using value_type = decltype([] {
         if constexpr (keys.size() < u8{}-1u) { return u8{}; }
    else if constexpr (keys.size() < u16{}-1u) { return u16{}; }
    else if constexpr (keys.size() < u32{}-1u) { return u32{}; }
    else if constexpr (keys.size() < u64{}-1u) { return u64{}; }
    else if constexpr (keys.size() < u128{}-1u) { return u128{}; }
  }());

  utility::array<utility::compressed_pair<key_type, value_type>, keys.size()> kv;
  for (value_type i{}; i < keys.size(); ++i) {
    kv[i] = utility::compressed_pair{to<key_type>(keys[i]), value_type(i + 1u)};
  }
  return kv;
}
template<const auto& keys>
constexpr auto kv() requires requires(u32 n) { keys[n].first.size(); keys[n].second; } {
  constexpr auto max_len = [] {
    u32 len{};
    for (const auto& key : keys) { if (key.first.size() > len) { len = key.first.size(); } }
    return len;
  }();

  using key_type = decltype([] {
        if constexpr (max_len <= sizeof(u8)) { return u8{}; }
   else if constexpr (max_len <= sizeof(u16)) { return u16{}; }
   else if constexpr (max_len <= sizeof(u32)) { return u32{}; }
   else if constexpr (max_len <= sizeof(u64)) { return u64{}; }
   else if constexpr (max_len <= sizeof(u128)) { return u128{}; }
  }());

  using value_type = typename type_traits::value_type_t<keys>::second_type;

  utility::array<utility::compressed_pair<key_type, value_type>, keys.size()> kv;
  for (auto i = 0; i < keys.size(); ++i) {
    kv[i] = utility::compressed_pair{to<key_type>(keys[i].first), keys[i].second};
  }
  return kv;
}
} // namespace detail

template<const auto& kv, auto... ts, const auto& kv_ = type_traits::constant_v<detail::kv<kv>()>.value>
  requires concepts::range<kv_>
[[nodiscard]] [[gnu::always_inline]]
constexpr auto hash(const auto& key) noexcept -> decltype(hash<kv_, ts...>(key)) {
  return hash<kv_, ts...>(key);
}
} // namespace mph

#if not defined(DISABLE_STATIC_ASSERT_TESTS)
static_assert(([] {
  constexpr auto expect = [](bool cond) { if (not cond) { void failed(); failed(); } };

  // mph::utility::compressed_pair
  {
    static_assert(1 == mph::utility::compressed_pair{1, 2}.first);
    static_assert(2 == mph::utility::compressed_pair{1, 2}.second);
    static_assert(sizeof(int) + sizeof(int) == sizeof(mph::utility::compressed_pair{int{}, int{}}));
    struct empty { };
    static_assert(sizeof(int) == sizeof(mph::utility::compressed_pair{empty{}, 42}));
    static_assert(sizeof(int) == sizeof(mph::utility::compressed_pair{42, empty{}}));
    static_assert(sizeof(empty) + sizeof(empty) == sizeof(mph::utility::compressed_pair{empty{}, empty{}}));
  }

  // mph::utility::array
  {
    {
      mph::utility::array<mph::u32, 1> a{};
      expect(1 == a.size());
    }

    {
      mph::utility::array a{1, 2};
      expect(2u == a.size());
      expect(1 == a[0]);
      expect(2 == a[1]);
    }

    {
      mph::utility::array a{1};
      a[0] = 2;
      expect(2 == a[0]);
    }

    {
      mph::utility::array a{1, 2, 3};
      expect(3u == a.size());
      expect(a.begin() != a.end());
      expect(a.size() == mph::u32(a.end() - a.begin()));
      expect(a.end() == a.begin() + a.size());
    }
  }

  // mph::utility::constant/_v
  {
    static_assert(0 == mph::type_traits::constant<0>::value);
    static_assert(0 == mph::type_traits::constant_v<0>);
    static_assert(42 == mph::type_traits::constant<42>{});
    static_assert(42 == mph::type_traits::constant_v<42>);
    static_assert(42u == mph::type_traits::constant<42u>::value);
    static_assert('X' == mph::type_traits::constant<'X'>::value);
    static_assert('X' == mph::type_traits::constant_v<'X'>);
  }

  // mph::type_traits::aligned_t
  {
    using mph::utility::array;

    static_assert(16u == alignof(mph::type_traits::aligned_t<array<int, 42>, 16u>));
    static_assert(32u == alignof(mph::type_traits::aligned_t<array<int, 42>, 32u>));
    static_assert(64u == alignof(mph::type_traits::aligned_t<array<int, 42>, 64u>));
    static_assert(alignof(int) == alignof(mph::type_traits::aligned_t<array<int, 42>, 0u>));
  }

  // mph::type_traits::conditional_t
  {
    using mph::u32;
    using mph::u64;

    static_assert(sizeof(u32) == sizeof(mph::type_traits::conditional_t<true, u32, u64>));
    static_assert(sizeof(u64) == sizeof(mph::type_traits::conditional_t<false, u32, u64>));
  }

  // mph::concepts::integral
  {
    static_assert(mph::concepts::integral<mph::u8>);
    static_assert(mph::concepts::integral<mph::u16>);
    static_assert(mph::concepts::integral<mph::u32>);
    static_assert(mph::concepts::integral<mph::u64>);
    static_assert(mph::concepts::integral<mph::u128>);
    static_assert(mph::concepts::integral<int>);
    static_assert(not mph::concepts::integral<float>);
    static_assert(not mph::concepts::integral<void>);
    static_assert(not mph::concepts::integral<const int&>);
    static_assert(not mph::concepts::integral<const char*>);
  }

  // mph::concepts::config
  {
    struct cfg1{ };
    struct cfg2{ int key_in_set_probability{}; };
    struct cfg3{ int key_in_set_probability{}; int group_size{}; };
    struct cfg4{ int key_in_set_probability{}; int group_size{}; int lookup_table_alignment{}; };

    static_assert(not mph::concepts::config<cfg1{}>);
    static_assert(not mph::concepts::config<cfg2{}>);
    static_assert(not mph::concepts::config<cfg4{.key_in_set_probability = -1, .group_size = {}, .lookup_table_alignment = {}}>);
    static_assert(not mph::concepts::config<cfg4{.key_in_set_probability = 100u, .group_size = {}, .lookup_table_alignment = {}}>);
    static_assert(mph::concepts::config<cfg4{.key_in_set_probability = 0u, .group_size = 1u, .lookup_table_alignment = {}}>);
    static_assert(mph::concepts::config<cfg4{.key_in_set_probability = 50u, .group_size = 4u, .lookup_table_alignment = {}}>);
    static_assert(mph::concepts::config<cfg4{.key_in_set_probability = 100u, .group_size = 8u, .lookup_table_alignment = {}}>);
  }

  // concepts::range
  {
    using mph::u32;
    using mph::type_traits::constant;

    struct range1 { };
    struct range2 { u32 begin() const; u32 end() const; };
    struct range3 { u32 begin() const; u32 end() const; u32 operator[](u32) const; };
    struct range4 { u32 begin() const; u32 end() const; u32 size() const; u32 operator[](u32) const; };

    static_assert(not mph::concepts::range<constant<range1{}>::value>);
    static_assert(not mph::concepts::range<constant<range2{}>::value>);
    static_assert(not mph::concepts::range<constant<range3{}>::value>);
    static_assert(mph::concepts::range<constant<range4{}>::value>);
  }

  // mph::detail::pext
  {
    using mph::u32;

    static_assert(0    == mph::detail::pext(0b00, 0b00));
    static_assert(0    == mph::detail::pext(0b01, 0b00));
    static_assert(0b1  == mph::detail::pext(0b01, 0b01));
    static_assert(0b01 == mph::detail::pext(0b01, 0b11));
    static_assert(0b0  == mph::detail::pext(0b01, 0b10));
    static_assert(0b1  == mph::detail::pext(0b11, 0b10));
    static_assert(0b1  == mph::detail::pext(0b11, 0b01));
    static_assert(0b11 == mph::detail::pext(0b11, 0b11));
  }

  // mph::detail::mask
  {
    using mph::u32;
    using mph::utility::array;
    using mph::utility::compressed_pair;

    static_assert(0b01 == mph::detail::mask<u32, 1u>(array{compressed_pair{0b10, 0}, compressed_pair{0b11, 0}}));
    static_assert(0b10 == mph::detail::mask<u32, 1u>(array{compressed_pair{0b01, 0}, compressed_pair{0b11, 0}}));
    static_assert(0b10 == mph::detail::mask<u32, 1u>(array{compressed_pair{0b11, 0}, compressed_pair{0b01, 0}}));
    static_assert(0b01 == mph::detail::mask<u32, 1u>(array{compressed_pair{0b11, 0}, compressed_pair{0b10, 0}}));
    static_assert(0b01 == mph::detail::mask<u32, 1u>(array{compressed_pair{0b01, 0}, compressed_pair{0b10, 0}}));
    static_assert(0b01 == mph::detail::mask<u32, 1u>(array{compressed_pair{0b10, 0}, compressed_pair{0b01, 0}}));
    static_assert(0b11 == mph::detail::mask<u32, 1u>(array{compressed_pair{0b11, 0}, compressed_pair{0b11, 0}}));
  }

  // mph::to
  {
    using mph::u32;
    using mph::utility::array;

    static_assert(0 == mph::to<u32>(0));
    static_assert(42 == mph::to<u32>(42));
    static_assert(42u == mph::to<u32>(42u));
    static_assert(0 == mph::to<u32>(""));
    static_assert((u32('A') << 0) + (u32('B') << __CHAR_BIT__) == mph::to<u32>("AB"));
    static_assert((u32('A') << 0) + (u32('B') << __CHAR_BIT__) + (u32('C') << __CHAR_BIT__*2) == mph::to<u32>("ABC"));
    static_assert((u32('A') << 0) + (u32('B') << __CHAR_BIT__) == mph::to<u32>(array{'A','B'}));
    static_assert((u32('A') << 0) + (u32('B') << __CHAR_BIT__) + (u32('C') << __CHAR_BIT__*2) == mph::to<u32>(array{'A','B','C'}));
  }

  // mph::hash
  {
    using mph::u32;
    using mph::utility::compressed_pair;
    using mph::utility::array;
    using mph::type_traits::constant;

    // integral
    {
      {
        constexpr const auto& kv = constant<array{
          compressed_pair{u32(4), u32(2)}
        }>::value;

        static_assert(0 == mph::hash<kv>(u32(0)));
        static_assert(0 == mph::hash<kv>(u32(1)));
        static_assert(2 == mph::hash<kv>(u32(4)));
      }

      {
        constexpr const auto& kv = constant<array{
          compressed_pair{u32(4), u32(2)},
          compressed_pair{u32(42), u32(87)},
          compressed_pair{u32(100), u32(100)},
        }>::value;

        static_assert(0 == mph::hash<kv>(u32(0)));
        static_assert(2 == mph::hash<kv>(u32(4)));
        static_assert(87 == mph::hash<kv>(u32(42)));
        static_assert(100 == mph::hash<kv>(u32(100)));
      }
    }

    // string-like
    {
      {
        constexpr const auto& kv = constant<array{
          compressed_pair{mph::to<u32>("BTC"), 1},
          compressed_pair{mph::to<u32>("ETH"), 2},
          compressed_pair{mph::to<u32>("XRP"), 3},
        }>::value;

        static_assert(0 == mph::hash<kv>(""));
        static_assert(1 == mph::hash<kv>("BTC"));
        static_assert(2 == mph::hash<kv>("ETH"));
        static_assert(3 == mph::hash<kv>("XRP"));
      }
    }
  }
}(), true));
#endif
#endif // MPH
